{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"IYICNLFoZxwT","executionInfo":{"status":"ok","timestamp":1714921812732,"user_tz":-330,"elapsed":2257,"user":{"displayName":"Adhithiyan Ravi","userId":"01423494443567589158"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"SK-8kiYFZxwU","executionInfo":{"status":"ok","timestamp":1714921814397,"user_tz":-330,"elapsed":5,"user":{"displayName":"Adhithiyan Ravi","userId":"01423494443567589158"}}},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/mllab/50_Startups.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2XJlGLdZxwU"},"outputs":[],"source":["def computeCost(X, y, theta):\n","    m = len(y)\n","    predictions = X.dot(theta)\n","    square_err = (predictions - y) ** 2\n","    return 1 / (2 * m) * np.sum(square_err)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4PCHvthZxwU"},"outputs":[],"source":["def gradientDescent(X, y, theta, alpha, num_iters):\n","    m = len(y)\n","    J_history = np.zeros(num_iters)\n","\n","    for i in range(num_iters):\n","        predictions = X.dot(theta)\n","        errors = np.dot(X.transpose(), (predictions - y))\n","        theta -= alpha * (1 / m) * errors\n","        J_history[i] = computeCost(X, y, theta)\n","\n","    return theta, J_history"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"j1CR2zZuZxwU","executionInfo":{"status":"ok","timestamp":1714921815923,"user_tz":-330,"elapsed":5,"user":{"displayName":"Adhithiyan Ravi","userId":"01423494443567589158"}}},"outputs":[],"source":["X = df[['R&D Spend']].values\n","y = df['Profit'].values.reshape(-1, 1)\n","X_b = np.c_[np.ones((len(X), 1)), X]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kix7fa0iZxwU"},"outputs":[],"source":["mean_X = np.mean(X)\n","std_X = np.std(X)\n","X_scaled = (X - mean_X) / std_X\n","X_b_scaled = np.c_[np.ones((len(X_scaled), 1)), X_scaled]\n","alpha = 0.01\n","num_iters = 1500\n","theta = np.zeros((2, 1))\n","theta, J_history = gradientDescent(X_b_scaled, y, theta, alpha, num_iters)\n","plt.plot(range(1, num_iters + 1), J_history, '-b', label='Cost J')\n","plt.xlabel('Number of iterations')\n","plt.ylabel('Cost')\n","plt.title('Convergence of Gradient Descent')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwQoqv9uZxwU"},"outputs":[],"source":["new_data_point = 1000\n","new_data_point_scaled = (new_data_point - mean_X) / std_X\n","new_data_point_b = np.array([1, new_data_point_scaled])\n","predicted_profit = np.dot(new_data_point_b, theta)\n","print(\"Predicted Profit for R&D Spend {}: {}\".format(new_data_point, predicted_profit[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SOjE9HLZxwV"},"outputs":[],"source":["new_data_point = 59555\n","new_data_point_scaled = (new_data_point - mean_X) / std_X\n","new_data_point_b = np.array([1, new_data_point_scaled])\n","predicted_profit = np.dot(new_data_point_b, theta)\n","print(\"Predicted Profit for R&D Spend {}: {}\".format(new_data_point, predicted_profit[0]))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}